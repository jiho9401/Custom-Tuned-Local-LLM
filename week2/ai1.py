import os
import streamlit as st
import re
from langchain.llms import Ollama
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage
import time
from langchain.prompts import PromptTemplate

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(current_dir)

# =======================
# 1. ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´ ì¶”ê°€
# =======================
MODEL_OPTIONS = {
    "ë¼ë§ˆ 3.2 3B": {"model": "llama33B8Q", "base_url": "http://192.168.10.123:11434"},
    "ë¼ë§ˆ 3 8B": {"model": "llama38B4Q", "base_url": "http://192.168.0.123:11434"},
    "deepseek-r1 32b": {"model": "deepseek-r1:32b", "base_url": None},  # ë¡œì»¬ ì‹¤í–‰
    "qwq": {"model": "qwq", "base_url": None},  # ë¡œì»¬ ì‹¤í–‰
}

st.title("LLM í…ŒìŠ¤íŒ…")

# =======================
# 2. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ìœ ì§€)
# =======================
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "ë¼ë§ˆ 3.2 3B"

# ëŒ€í™” ê¸°ì–µì„ ìœ„í•œ ë©”ëª¨ë¦¬ (í•œ ë²ˆë§Œ ìƒì„±)
if "memory" not in st.session_state:
    # ê¸´ ë©”ëª¨ë¦¬ ìœ ì§€í•˜ë©´ì„œ ê°€ì¤‘ì¹˜ ì¡°ì ˆì„ ìœ„í•œ ì„¤ì •
    st.session_state.memory = ConversationBufferWindowMemory(k=5000, return_messages=True, memory_key="history")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ì„¸ì…˜ ìƒíƒœì— ì €ì¥)
if "template" not in st.session_state:
    st.session_state.template = """ë‹¹ì‹ ì€ ì§€ì‹ì´ í’ë¶€í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ìš°ë¦¬ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:
{history}

ê°€ì¥ ìµœê·¼ì˜ ì§ˆë¬¸ì€ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•˜ì„¸ìš”. 
ì´ì „ ëŒ€í™”ëŠ” ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë° ì°¸ê³ ë§Œ í•˜ê³ , í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.

í˜„ì¬ ì§ˆë¬¸: {input}

ë‹µë³€:"""

selected_model = st.selectbox(
    "ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
    options=list(MODEL_OPTIONS.keys()),
    index=list(MODEL_OPTIONS.keys()).index(st.session_state.selected_model),  # ê¸°ë³¸ê°’ ìœ ì§€
)

# ì„ íƒí•œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
selected_model_info = MODEL_OPTIONS[selected_model]
model_name = selected_model_info["model"]
base_url = selected_model_info["base_url"]

# =======================
# 3. ëª¨ë¸ ë³€ê²½ ì‹œ ê¸°ì¡´ memory ìœ ì§€í•˜ë©´ì„œ ConversationChain ì—…ë°ì´íŠ¸
# =======================
if "conversation_chain" not in st.session_state or st.session_state.selected_model != selected_model:
    st.session_state.selected_model = selected_model  # ì„ íƒ ëª¨ë¸ ì €ì¥

    # ìƒˆë¡œìš´ LLM ìƒì„±
    if base_url:  # ì›ê²© ì„œë²„ ì‚¬ìš©
        llm = Ollama(model=model_name, base_url=base_url)
    else:  # ë¡œì»¬ ì‹¤í–‰
        llm = Ollama(model=model_name)

    # ì„¸ì…˜ ìƒíƒœì˜ í…œí”Œë¦¿ ì‚¬ìš©
    prompt = PromptTemplate(
        input_variables=["history", "input"],
        template=st.session_state.template
    )

    # ê¸°ì¡´ memoryë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
    st.session_state.conversation_chain = ConversationChain(
        llm=llm, 
        memory=st.session_state.memory,
        prompt=prompt,
        verbose=False
    )

# =======================
# 4. ëŒ€í™” ë‚´ì—­ í‘œì‹œ
# =======================
for message in st.session_state.memory.chat_memory.messages:
    if isinstance(message, HumanMessage):
        st.chat_message("user").write(message.content)
    elif isinstance(message, AIMessage):
        clean_response = re.sub(r"<think>.*?</think>", "", message.content, flags=re.DOTALL).strip()  # <think> íƒœê·¸ ì œê±°
        st.chat_message("assistant").write(clean_response)

# =======================
# 5. ì±„íŒ… ì…ë ¥ì°½ (í•˜ë‹¨ ê³ ì •)
# =======================
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:"):
    st.chat_message("user").write(prompt)  # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥

    if "conversation_chain" in st.session_state:
        ai_msg = st.chat_message("assistant")
        
        # ìƒê°ê³¼ ì‘ë‹µì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
        thinking_container = ai_msg.container()
        response_container = ai_msg.container()
        
        # ì‘ë‹µ í‘œì‹œë¥¼ ìœ„í•œ ë¹„ì–´ìˆëŠ” í…ìŠ¤íŠ¸ ì˜ì—­ ìƒì„±
        with response_container:
            response_area = st.empty()
        
        # ìƒíƒœ í‘œì‹œ
        with st.status("ğŸ¤” AIê°€ ìƒê° ì¤‘...", expanded=True) as status:
            response = ""
            thinking = ""
            is_thinking = False
            
            # ê³¼ê±° ëŒ€í™” ë‚´ìš© í˜•ì‹ ë³€í™˜ (í”„ë¡¬í”„íŠ¸ìš©)
            past_messages = st.session_state.memory.chat_memory.messages
            history_text = ""
            
            for msg in past_messages:
                if isinstance(msg, HumanMessage):
                    history_text += f"Human: {msg.content}\n"
                elif isinstance(msg, AIMessage):
                    # <think> íƒœê·¸ ì œê±°
                    clean_content = re.sub(r"<think>.*?</think>", "", msg.content, flags=re.DOTALL).strip()
                    history_text += f"AI: {clean_content}\n"
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            prompt_with_history = st.session_state.template.format(
                history=history_text,
                input=prompt
            )
            
            for chunk in st.session_state.conversation_chain.llm.stream(prompt_with_history):
                # <think> íƒœê·¸ ì²˜ë¦¬
                if "<think>" in chunk:
                    is_thinking = True
                    # thinking_containerì— ìƒê° ì˜ì—­ ìƒì„±
                    with thinking_container:
                        st.markdown("#### ğŸ§  ëª¨ë¸ì´ ìƒê°í•˜ëŠ” ê³¼ì •:")
                        st.markdown("---")
                        thinking_placeholder = st.empty()
                    continue
                    
                if "</think>" in chunk:
                    is_thinking = False
                    with thinking_container:
                        st.markdown("---")
                    continue
                
                if is_thinking:
                    # ìƒê° ë‚´ìš© ì¶”ê°€
                    thinking += chunk
                    # ìƒê° ê³¼ì • í‘œì‹œ (ê¸°ìš¸ì„ì²´ ì‚¬ìš©ìœ¼ë¡œ êµ¬ë¶„)
                    with thinking_container:
                        thinking_placeholder.markdown(f"*{thinking}*")
                else:
                    # ì‘ë‹µ ì¶”ê°€ - ì •ê·œí™” ì½”ë“œ ì œê±°í•˜ê³  ì›ë³¸ ìœ ì§€
                    response += chunk
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (HTML ê°•ì œ)
                    response_area.markdown(f"<div class='response-text'>{response}</div>", unsafe_allow_html=True)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            status.update(label="âœ… ì‘ë‹µ ì™„ë£Œ!", state="complete", expanded=False)

        # ìµœì¢… ì‘ë‹µ ì €ì¥
        if thinking:
            full_response = f"<think>{thinking}</think>{response}"
        else:
            full_response = response
            
        # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        st.session_state.memory.chat_memory.add_user_message(prompt)
        st.session_state.memory.chat_memory.add_ai_message(full_response)
        
    else:
        st.error("âš ï¸ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        
# í˜ì´ì§€ ìŠ¤íƒ€ì¼ ê°œì„ 
st.markdown("""
<style>
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    [data-testid="stChatMessageContent"] {
        border-radius: 12px;
    }
    .response-text {
        line-height: 1.5;
        letter-spacing: normal !important;
        font-family: 'Source Sans Pro', sans-serif !important;
        font-size: 1rem !important;
    }
</style>
""", unsafe_allow_html=True)
