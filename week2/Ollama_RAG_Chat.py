import streamlit as st
import requests
import json
import os
from langchain.memory import ConversationBufferWindowMemory
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Ollama ì„œë²„ ì •ë³´
OLLAMA_SERVER = "http://localhost:11434"
LLM_MODEL = "llama384"
MEMORY_FILE = "chat_memory.json"
VECTOR_DB_PATH = "faiss_index"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Ollama Chat", page_icon="ğŸ’¬", layout="wide")

# ë©”ëª¨ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_memory():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "r") as f:
            return json.load(f)
    return []

# ë©”ëª¨ë¦¬ ì €ì¥ í•¨ìˆ˜
def save_memory():
    with open(MEMORY_FILE, "w") as f:
        json.dump(st.session_state.messages, f)

# PDF íŒŒì¼ì„ ì½ì–´ì™€ì„œ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def load_pdfs_from_folder(folder_path="data_pdf"):
    documents = []
    if os.path.exists(folder_path):
        for file_name in os.listdir(folder_path):
            if file_name.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, file_name)
                loader = PyPDFLoader(pdf_path)
                documents.extend(loader.load())
    return documents

# PDF ë‚´ìš©ì„ í˜ì´ì§€ë³„ë¡œ ì„¸ë¶„í™”í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def process_pdfs():
    documents = load_pdfs_from_folder()
    if documents:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        split_docs = []
        for doc in documents:
            split_docs.extend(text_splitter.split_text(doc.page_content))
        return split_docs
    return []

# RAGìš© VectorDB ìƒì„± í•¨ìˆ˜
def create_vector_db():
    if not os.path.exists(VECTOR_DB_PATH):
        documents = process_pdfs()
        if documents:
            embeddings = HuggingFaceEmbeddings()
            vector_db = FAISS.from_texts(documents, embeddings)
            vector_db.save_local(VECTOR_DB_PATH)
        else:
            st.error("data_pdf í´ë”ì— PDF íŒŒì¼ì´ ì—†ê±°ë‚˜ íŒŒì‹±ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

# VectorDB ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_vector_db():
    if os.path.exists(VECTOR_DB_PATH):
        embeddings = HuggingFaceEmbeddings()
        return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        create_vector_db()
        return load_vector_db()

# LangChain ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœì— ì €ì¥)
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferWindowMemory(k=50, human_prefix="User", ai_prefix="Assistant")
    st.session_state.messages = load_memory()
    st.session_state.vector_db = load_vector_db()

# ì±„íŒ… UI êµ¬ì„± (ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ)
st.title("ğŸ’¬ Ollama AI Chat with Enhanced Memory & RAG")

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ì°½ (ì•„ë˜ ë°°ì¹˜)
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")

# ì‚¬ìš©ìê°€ ì…ë ¥í•˜ë©´ Ollamaì™€ í†µì‹ 
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})

    # LangChain ë©”ëª¨ë¦¬ì— ì €ì¥
    st.session_state.memory.save_context({"input": user_input}, {"output": ""})

    # RAGë¥¼ ì´ìš©í•œ ì •ë³´ ê²€ìƒ‰
    search_results = st.session_state.vector_db.similarity_search(user_input, k=5)
    retrieved_context = "\n".join([doc.page_content for doc in search_results])

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    with st.chat_message("user"):
        st.markdown(user_input)

    # LangChain ë©”ëª¨ë¦¬ì—ì„œ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    chat_history = st.session_state.memory.load_memory_variables({})["history"]

    # Ollama API ìš”ì²­ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
    url = f"{OLLAMA_SERVER}/api/generate"
    headers = {"Content-Type": "application/json"}

    # Ollamaì— ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ (ëŒ€í™” íˆìŠ¤í† ë¦¬, ìš”ì•½, RAG í¬í•¨)
    full_prompt = f"""ìš”ì•½ëœ ëŒ€í™”:
{chat_history}

ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´:
{retrieved_context}

User: {user_input}
Assistant:"""

    payload = {
        "model": LLM_MODEL,
        "prompt": full_prompt,
        "stream": True
    }

    # AI ì‘ë‹µì„ ì¶œë ¥í•˜ëŠ” ê³µê°„ í™•ë³´
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        with requests.post(url, headers=headers, json=payload, stream=True) as response:
            for line in response.iter_lines():
                if line:
                    json_data = json.loads(line.decode("utf-8"))
                    if "response" in json_data:
                        full_response += json_data["response"]
                        message_placeholder.markdown(full_response + "â–Œ")

        # ìµœì¢… ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥
        st.session_state.memory.save_context({"input": user_input}, {"output": full_response})

        # ì±„íŒ… ê¸°ë¡ì—ë„ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": full_response})

        # ë©”ëª¨ë¦¬ ì €ì¥
        save_memory()

        # ì»¤ì„œ ì œê±° í›„ ìµœì¢… ì‘ë‹µ í‘œì‹œ
        message_placeholder.markdown(full_response)

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ğŸ’¾ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"):
    st.session_state.memory.clear()
    st.session_state.messages = []
    if os.path.exists(MEMORY_FILE):
        os.remove(MEMORY_FILE)
    if os.path.exists(VECTOR_DB_PATH):
        os.system(f"rm -rf {VECTOR_DB_PATH}")
    st.success("ë©”ëª¨ë¦¬ì™€ VectorDBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
