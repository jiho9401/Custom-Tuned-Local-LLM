import streamlit as st
import requests
import json

# Ollama ì„œë²„ ì •ë³´
OLLAMA_SERVER = "http://localhost:11434"
LLM_MODEL = "llama3.2:1b"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Ollama Chat", page_icon="ğŸ’¬", layout="wide")

# ì±„íŒ… ê¸°ë¡ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… UI êµ¬ì„± (ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ)
st.title("ğŸ’¬ Ollama AI Chat")

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ì°½ (ì•„ë˜ ë°°ì¹˜)
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")

# ì‚¬ìš©ìê°€ ì…ë ¥í•˜ë©´ Ollamaì™€ í†µì‹ 
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    with st.chat_message("user"):
        st.markdown(user_input)

    # Ollama API ìš”ì²­ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
    url = f"{OLLAMA_SERVER}/api/generate"
    headers = {"Content-Type": "application/json"}
    payload = {"model": LLM_MODEL, "prompt": user_input, "stream": True}

    # AI ì‘ë‹µì„ ì¶œë ¥í•˜ëŠ” ê³µê°„ í™•ë³´
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
        with requests.post(url, headers=headers, json=payload, stream=True) as response:
            for line in response.iter_lines():
                if line:
                    # JSON ë””ì½”ë”©
                    json_data = json.loads(line.decode("utf-8"))
                    if "response" in json_data:
                        full_response += json_data["response"]
                        message_placeholder.markdown(full_response + "â–Œ")

        # ìµœì¢… ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        message_placeholder.markdown(full_response)  # ì»¤ì„œ ì œê±°